
CS244, Randomness and Computation

:00
Reminders

	PS8 due tonight

	Questions? office hours 1:30-3 today as usual


:02
Multiplication of rectangular matrices

	A*B is defined as long as A is nxm and B is mxp; the result is nxp

	computation of A*B may be described in two alternative ways:

	1) outer product method: A*B is the sum of the k outer products 
	k-th col(A) * k-th row(B) for k=1...m

	2) inner product method: (A*B)(i,j) is the inner product irow(A)*jcol(B)


:05
Special cases of matrix multiplication

	A*col combines columns of A with elements of col as weights

	>> A = randi(10,2,3)

	A =

     	4     3     5
     	6     7     2

	>> A*[1;2;3]

	ans =

    	25
    	26


	row*A combines rows of A with elements of row as weights

	>> [1 2]*A

	ans =

    	16    17     9


:10
Examples of matrix multiplication

	1) recenter columns of matrix A so that each has mean 0

	first try:

		A - mean(A,1) doesn't work (dimension mismatch)

	how to get appropriate mean to fill entire column?

	use outer product:

		A - ones(size(A,1),1)*mean(A,1)

	2) scale columns so that each has standard deviation 1

		std(A) operates on columns, so:

		A*(1./std(A))


:20
Matrix transposes

	Recall that the transpose of a matrix A is the matrix A' for which

		A'(i,j) = A(j,i)

	A' has the columns of A as its rows, and the rows of A as its columns

	Example:	A = [1 2; 3 4] => A' = [1 3; 2 4]

	It is easy to see that (A + B)' = A' + B'


:25
How does matrix multiplication interact with transposition?

	We ask: (A*B)' = ?

	Since any matrix product is a sum of outer products of vectors,
	we look at those building blocks first:

		v out* w has (i,j) element v(i)w(j)

		therefore

		(v out* w)' has (i,j) element v(j)w(i), same as for w out* v,
		where each vector has been transposed to allow outer product

		that is, (v out* w)' = (w' out*	v'),

		or (v*w)' = w'*v' in the case of outer products of vectors

	The transpose for a general matrix product follows from this case
	(leave giving the details shown below as exercise)

		(A*B)' = (sum_k A(:,k)*B(k,:))' = sum_k B(k,:)'*A(:,k)'
			= sum_k B'(:,k)*A'(k,:)
			= B'*A'


:32
Matrices as linear transformations

	An nxm matrix A defines a transformation 
		between vectors of length m and vectors of length n

	by either left multiplication on column vectors:

		L_A: column -> A*column

	or right multiplication on row vectors:

		R_A: row -> row*A

	These are linear transformations:

		T_A(v + w) = T_A(v) + T_A(w)

		T_A(c v) = c T_A(v)

	In particular, T_A(0) = 0

	
:38
Examples of 2D linear transformations by 2x2 matrices

	(or just use eigshow at this point, given discussion from last time)

	axis scaling

		[2 0; 0 5]

	reflections

		[-1 0; 0 1]

	rotations

		[c s; -s c]

	shear transformations

		[1 0;2 1]

	translations are not linear - why?

	illustrate these in MATLAB, using my shape() function


:40
Example: covariance matrix in terms of matrix multiplication

	if you have n experimental samples of X, Y, as rows of a nx2 matrix, 
	with X values in the first column and Y values in the second column:

		D = [x1 y1; x2 y2; x3 y3]

	and you want to estimate the covariance matrix:

		covM(X,Y) = 	[E((X-E(X)^2) 		E((X-E(X))(Y-E(Y)))

		     		E((Y-E(Y))(X-E(X))) 	E((Y-E(Y))^2)]

	you can rewrite in matrix  multiplication terms as:

		covM(X,Y) = E of [X-E(X), Y-E(Y)]'*[X-E(X) Y-E(Y)]

	Work out in class


:45
Effect of a linear transformation on the covariance matrix

	Suppose that [X Y] is a 2D RV with covariance matrix C

	Suppose that A is a 2x2 matrix

	Define a new 2D RV by matrix multiplication:

		[U V] = [X Y]*A

	How is the covariance matrix of [U V] determined in terms of C, A?

	We assume that [X Y] has expected value 0:

		[E(X) E(Y)] = [0 0]

	First calculate the expected value of [U V]:

		[E(U) E(V)] = E([U V]) = E( [X Y]*A ) = E([X Y])*A = [0 0]

	Nice. Now compute the covariance matrix of the new variables:

		Cnew = E([U V]*[U V]') = E( ([X Y]*A)'*[X Y]*A )

			= E( A'*[X Y]'*[X Y]*A )

			= A'*E([X Y]'*[X Y])*A 

			= A'*C*A


:50
Example of the covariance matrix after a linear transformation

	Do one or two examples in MATLAB

	xy = randn(10^3,2);		independent Gaussians with mean 0, var 1

	scatter(xy(:,1), xy(:,2));	nice and symmetrical

	cov(xy);			what do you expect the covariance to be?

	A = some 2x2 matrix with a clear geometric meaning (e.g., shear)

	z = xy*A;

	scatter(w(:,1), w(:,2));	transformed according to A

	cov(z);				take a look

	A'*cov(xy)*A;			any similarities?


:55
Eigenvectors

	For a given matrix A, certain vectors v are special in that the 
	transformation T_A only scales them; it does not rotate them at all
	(although a complete reversal is allowed, as it is a negative scaling)

	Algebraically:

		T_A(v) = cv for some constant c

	Such vectors are called characteristic vectors, proper vectors,
	or, traditionally, the German eigenvectors 

	The constant c is known as an eigenvalue of A, and goes with v

		"v is an eigenvector of A with eigenvalue c"


:60
Example of eigenvectors

	Consider the matrix A = [1 2;0 1] acting by right multiplication
	on row vectors (examine in MATLAB)

	This is a shear transformation, as its action on a rectangle shows

		you can guess what vectors don't change direction

	Let's look at the details:

		[x y][1 2; 0 1] = [x y+2x]

	Clearly, if x=0, then R_A(v) = v, so any vector [0 y]
	is an eigenvector of R_A

	The vectors [0 y] are also called left eigenvectors of the matrix A
	(R_A(v) is defined by placing A on the right, and v on the left)

	Does L_A have the same eigenvectors as R_A?

		[1 2; 0 1][x; y] = [x+2y y]

	so eigenvectors of L_A are of the form [x 0]

	The vectors [x 0] are called right eigenvectors of A,
	as their direction remains invariant when they are "to the right of A"


:if time allows
Matrix transposes and eigenvectors

	Suppose that v is a right eigenvector of A:

		A*v = cv

	Take transposes:

		v'*A' = cv'

	This shows that v' is a left eigenvector of A'

	Obviously, the correspondence goes both ways:

		the left eigenvectors of a matrix A are the transposes of 
		the right eigenvectors of the transposed matrix A', and
		the corresponding eigenvalues are the same in both cases


:next time
Eigenvectors of the covariance matrix

