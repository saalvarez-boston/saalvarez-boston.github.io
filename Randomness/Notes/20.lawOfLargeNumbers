
CS244, Randomness and Computation

:00
Discuss PS9 solutions


:20
Quick recap: principal components analysis in high dimensions

	X = m x n data matrix with one instance per column

	if all variables (rows) have mean 0, covariance is rescaled version of:

		C = X*X'

	standard cov(X) will be a huge matrix (e.g., for a 200x200 pixel image,
	cov(X) would be of size 40000 x 40000, that is, 1.6 billion elements!)

	Instancewise covariance X'*X will generally be a much smaller matrix

	If v is an eigenvector of the instancewise covariance matrix:

		X'*X*v = cv and v nonzero

	then the new vector w=X*v is an eigenvector of the standard covariance:

		X*X'*w = X*X'*X*v = X*(cv) = cX*v = cw

	The eigenvectors X*v are mutually orthogonal:

		X*v1 dot X*v2 = v1*X'*X*v2 = c2 v1*v2 = 0

	They can then be standardized to have length 1, and PCA can proceed
	as in the 2D case discussed earlier

	Compression is achieved by projecting onto the subspace generated by
	the eigenvectors with the largest eigenvalues


:25
Higher-D PCA example: imagewise covariance

	Load images from the Yale Face Database

		[X w h] = load_images('facelist.txt');

	Crop for improved results

		X = crop(X, w, h, w/2, h);

	Calculate mean for each pixel and shift all variables to 0 mean:

		means = mean(X,2);
		X = X - means;
	
	Calculate eigenvectors of mean-centered imagewise covariance:

		[vecs vals] = eig(X'*X);

	Sort in increasing order of eigenvalues:

		vals = flipud(fliplr(vals));

		vecs = fliplr(vecs);


:35
Higher-D PCA by example: eigenvectors of the pixelwise covariance

	Transform into eigenvectors of the pixelwise covariance:

		imvecs = X*vecs;

	Normalize eigenvectors:

		for i=1:15
			imvecs(:,i) = imvecs(:,i) / norm(imvecs(:,i));
		end

	Now the pixelwise eigenvectors form an orthonormal set

	Careful with the last eigenvector - you've normalized a "noise" vector,
	so the values are not to be trusted

		imvecs = imvecs(:,1:14);

	Reconstruct an image from its projections onto eigenvectors:

		reconstruct(X(:,5)+means, w/2, h, imvecs, means, 1);


:40
1D Random Processes

	Some phenomena involve uncertainty and variation over time

		heart rate signal

		stock price

	We can model such phenomena using sequences of RVs

		X1, X2, ... Xt, ....

	where Xt is a measurement of interest at time t

	We will assume that time is discrete, with t a non-negative integer

	Such a sequence is known as a random process (stochastic process)

		
:43
Example: repeated measurement of a random variable

	Any RV X produces randomly selected values according to some PMF

	If X is sampled repeatedly, a sequence of such values is produced

		for example, toss a coin 100 times

		measure the length of the line 100 times

		etc.

	Assuming that this multiple sampling experiment is repeated many times,
	the i-th value in the sequence may be viewed as a RV in its own right

		X1, X2, ..., Xi, ... X100

	Since each of these observations originates from the same original RV X,
	the RVs Xi will all have the same PMF - the same one as X

	We will also assume that the various samples are taken independently,
	so that the RVs Xi are independent of one another

	The Xi are said to be independent, identically distributed (iid) RVs

	This is the simplest example of what is known as a stationary
	random process, which involves no detectable change over time


:50
Laws of large numbers

	We have become accustomed to viewing the expected value of a RV X as 
	the expected average value of a large number of observations of X

	This idea can be formalized in terms of an iid sequence of RVs

	Indeed, the observations of X are an iid sequence Xi as described above

	We consider the relationship between the sample average

		Xn~ = (X1 + X2 + ... + Xn) / n

	and the expected value E(X)

	We expect that the sample average will be close to the expected value,
	especially if the number of samples, n, is very large


:53
Law of Large Numbers

	(LLN, weak form) If Xi is an iid sequence of RVs with finite variance,
	then the probability that Xn~ and E(X) are more than a distance d apart 
	decreases toward 0 as n becomes very large:

		lim_{n -> infinity} P(|Xn~ - E(X)| > d) = 0

	(LLN, strong form) If Xi is an iid sequence of RVs with finite variance,
	then the probability that Xn~ converges to E(X) as n -> infinity is 1

		P( lim_{n -> infinity} Xn~ = E(X) ) = 1


:60
Non-IID random processes

	It is more interesting to consider what happens when the various RV Xt
	that comprise a random process are dependent in some way

	We will study a particular form of dependence, namely that in which
	each RV Xt depends on the value of the RV X(t-1) immediately before Xt

		e.g., Xt = number of people in line at a store

			more likely that Xt=10 if X(t-1)=9 than if X(t-1)=0


:63
Next time: Markov chains

	We assume that there is a shared set of possible values of the Xt

	and that the probability that X(t)=vj is observed at time t is 
	determined by what value X(t-1) is observed at time t-1, according 
	to the same table of conditional probabilities for all t:

		p(i,j) = P(X(t)=i | X(t-1)=j)

	The p(i,j) are known as transition probabilities, and are frequently
	represented as arc weights on a graph that has the possible values vj
	as its nodes

	
:68
Temporal variation in a Markov chain

	If the value X(0)=v at time 0 is known, then the value at time X(1)
	will still be unknown, but we know its probability distribution:

		P(X(1)=i) = P(X(1(=i | X(0)=v) = p(i,column for v)

	In fact, we will know the PMF for X(1) even if we don't know X(0),
	as long as we know the PMF for X(0):

		P(X(1)=i) = sum_j P(X(1)=i | X(0)=j)*P(X(0)=j)

	This can be written as a matrix multiplication:

		P_X(1) = A*P_X(0)

	where A is the matrix of transition probabilities

