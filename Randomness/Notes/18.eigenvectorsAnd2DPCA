
CS244, Randomness and Computation

:00
Announcements

	PS9 due on Tuesday

	Questions? Office hours today and Tuesday, 1:30-3 as usual


:02
Eigenvectors

	For a given matrix A, certain vectors v are special in that the 
	transformation T_A only scales them; it does not rotate them at all
	(although a complete reversal is allowed, as it is a negative scaling)

	Geometrically:

		eigshow

	Algebraically:

		T_A(v) = cv for some constant c

	Such vectors are called characteristic vectors, proper vectors,
	or, traditionally, the German eigenvectors 

	The constant c is known as an eigenvalue of A, and goes with v

		"v is an eigenvector of A with eigenvalue c"


:06
Example of eigenvectors

	Consider the matrix A = [1 2;0 1] acting by right multiplication
	on row vectors (examine in MATLAB)

	This is a shear transformation, as its action on a rectangle shows

		you can guess what vectors don't change direction

	Let's look at the details:

		[x y][1 2; 0 1] = [x y+2x]

	Clearly, if x=0, then R_A(v) = v, so any vector [0 y]
	is an eigenvector of R_A

	The vectors [0 y] are also called left eigenvectors of the matrix A
	(R_A(v) is defined by placing A on the right, and v on the left)

	Does L_A have the same eigenvectors as R_A?

		[1 2; 0 1][x; y] = [x+2y y]

	so eigenvectors of L_A are of the form [x 0]

	The vectors [x 0] are called right eigenvectors of A,
	as their direction remains invariant when they are "to the right of A"


:12
Matrix transposes and eigenvectors

	Suppose that v is a right eigenvector of A:

		A*v = cv

	Take transposes:

		v'*A' = cv'

	This shows that v' is a left eigenvector of A'

	Obviously, the correspondence goes both ways:

		the left eigenvectors of a matrix A are the transposes of 
		the right eigenvectors of the transposed matrix A', and
		the corresponding eigenvalues are the same in both cases


:17
Eigenvectors and eigenvalues in MATLAB

	The eig function computes right eigenvectors and eigenvalues

		the vectors are normalized to have geometric length 1

	If you need left eigenvectors, just use eig on the transpose

	[vecs vals] = eig(matrix)

	>> A = [2 -1; -2 3]

	A =

     	2    -1
    	-2     3

	>> [vecs vals] = eig(A)

	vecs =

   	-0.7071    0.4472
   	-0.7071   -0.8944


	vals =

     	1     0
     	0     4


:20
Best linear fit to a scatterplot, revisited

	Consider a joint distribution over 2 RVs

	For example,

		xy = randn(10^3,2)*A;

	where A is a 2x2 matrix that defines some linear transformation

		A = [2 3; 0 5], say

	Look at the scatterplot of xy and the least squares linear fit:

		scatter(xy(:,1),xy(:,2));

	By the way, can you predict the covariance matrix of xy?

		recall that it should be A'cov(randn(.,2)A...

		C = cov(xy)

	Compare covariance to prediction, then continue to plot LS fit line:

		x=-10:.1:10; 
		y = C(1,2)/C(1,1)*x + mean(xy(:,2)-C(1,2)/C(1,1)*x);
		hold on;
		plot(x,y);

	While this line minimizes the mean squared error measured vertically,
	it doesn't appear to fit the geometry of the 2D scatterplot that well


:25
Matrix representations depend on the coordinate system

	A given linear transformation is represented by different 
	matrices in different coordinate systems

	For example, consider the simple transformation that
	scales the x axis by 2 and leaves y unchanged:

		T([x y]) = [2x y]

	Geometrically, it is clear that this transformation stretches
	objects horizontally by a factor of 2

	Represented as a matrix, we have:

		T = T_A, where A = [2 0; 0 1]

	Now view the same transformation after turning your head clockwise
	by one quarter of a turn, so that the new first coordinate (new x axis) 
	is what used to be the negative y axis, and the new second coordinate 
	(new y axis) is what used to be the x axis

	From the new point of view, the transformation T stretches the y axis
	by a factor of two, and leaves the x axis unchanged:

		T([x' y']) = [x 2y']

	The corresponding matrix representation is:

		T = T_B, where B = [1 0; 0 2]

	Thus, the matrix representing T is different, as the transformation
	appears to affect the axes differently (of course it's just that
	we're using different axes)


:30
Improving the 2D fit by geometric transformation

	We'll use the dependence of a matrix representation on the coordinates 
	to find a better fit to a scatterplot

	If the source data are Gaussian, the scatterplot will look elliptical

	Imagine that you somehow find an ideal linear fit to the scatterplot

	Imagine the appearance of the scatterplot as viewed from the fit line;
	that is, tilt your head so that the fit line is horizontal

	The scatterplot ellipse will be aligned with the new coordinate axes

	What will the covariance matrix of the rotated data look like?

		it should be diagonal, with 0 covariance between the new axes

	So, we should be able to find the best fit by finding a rotated 
	set of axes in which the covariance matrix is diagonal


:33
Diagonal matrices have simple eigenvectors

	Consider a 2x2 matrix, A

	View A as defining a linear transformation on points [x y]:

		T_A([x y]) = [x y]*A

	If A happens to be diagonal, say A = diag(d1,d2), then

		T_A([x y]) = [x y]*diag(d1,d2) = [d1*x d2*y]

	In particular, vectors of the special forms [x 0] and [0 y]
	are transformed by A into multiples of themselves:

		T_A([x 0]) = d1[x 0]

		T_A([0 y]) = d2[0 y]

	In other words, the axis vectors are eigenvectors of any diagonal matrix


:36
Matrices with simple eigenvectors are diagonal

	Maybe the eigenvectors of a matrix A are the key to making A diagonal?
	Suppose that you have a matrix A and eigenvectors v, w of A, that is, 
	v,w are nonzero and transformed by A into multiples of themselves:

		v*A = T_A(v) = c*v for some constant c

		w*A = T_A(v) = d*v for some constant d

	Then if you consider v,w to define new axes for a coordinate system,
	so that the new coordinates of a vector z are numbers a,b that satisfy:

		z = a*v + b*w,

	you will have by linearity:

		T_A(z) = a*T_A(v) + b*T_A(w) = a*cv + b*dw = [a b]*diag(c,d),

	which means that T_A is represented by the diagonal matrix diag(c,d)
	in this new coordinate system


:43
Matrix diagonalization

	A given linear transformations is represented by different matrices
	in different coordinate systems

	Given a particular matrix, A, we seek a coordinate system in which
	the linear transformation T_A is represented by a diagonal matrix

	By the above discussion, we know that the axes of such a coordinate
	system will be eigenvectors of T_A

	So, we find the eigenvectors, and we have our coordinate system!

	By our initial discussion, if A happens to be the covariance matrix
	of a pair of RVs, then these eigenvectors should be the geometric axes 
	("principal axes") of the scatterplot of a large sample of these RVs


:45
Example

	Do in MATLAB

	Use xy = randn(10^3,2)*[2 3; 0 5] as in the previous example

	Calculate the eigenvectors of the covariance matrix

		[vecs vals] = eig(cov(z));

	Superimpose the lines defined by the eigenvectors on the scatterplot

	These lines are precisely the principal axes!

	
	function [ output_args ] = drawPrincipalAxes( xyData )
	%DRAWPRINCIPALAXES draws principal axes on scatterplot of 2D data
	%xyData is n x 2 matrix with one 2D data sample per row
	%Sergio A. Alvarez
    		scatter(xyData(:,1), xyData(:,2)); 
    		axis equal;
    		hold on;
    		[vecs vals] = eig(cov(xyData));
    		t = -3:0.1:3;   % standard steps from mean
    		plot(t*sqrt(vals(1,1))*vecs(1,1), t*sqrt(vals(1,1))*vecs(2,1), ...
			'g', 'LineWidth', 2);
    		plot(t*sqrt(vals(2,2))*vecs(1,2), t*sqrt(vals(2,2))*vecs(2,2), ...
			'g', 'LineWidth', 2);
    		hold off;
	end


:50
Subspace projections

	In the above example, each vector in the scatterplot decomposes
	into a sum of two parts, one along each principal axis

		w = p1 v1 + p2 v2

	How can one find the projection factors, p1 and p2?

	We have the triangle interpretation of the inner product of two vectors:

		a dot b = row a * col b = |a||b| cos angle(a,b)
		
	Dot of a vector a with itself is easy:

		a dot a = |a|^2

	as is the dot of two mutually orthogonal (perpendicular) vectors:

		a dot b = 0 if a, b perpendicular to each other

	Since v1 and v2 are orthogonal, they have 0 as their inner product:

		v1 dot v2 = 0

	Take dot of w with v1:

		w dot v1 = p1 |v1|^2 

	so, the projection factor pops out:

		p1 = w dot v1 / |v1|^2

	likewise,

		p2 = w dot v2 / |v2|^2

	These formulas simplify further if v1 and v2 have length 1:

		pi = w dot vi


:60
Subspace projections example

	Continue the previous MATLAB example with A = [2 3; 0 5]

	We found the eigenvectors of the covariance matrix of randn(.,2)*A:

		orthonormal pair v, w

	What are the projections of the vector [1 2] onto these new axes?

		cv = v-projection = [1 2]*v

		cw = w-projection = [1 2]*w

	Assemble the resulting combination:

		z = cv*v + cw*w

		this yields z = [1 2], as it should!


:next time
n-dimensional random variables

	We'll next start looking at nD RV, for n >= 3

	Why would we care?

		Lots of data are higher-D (e.g., images, signals)


:next time
Basic notions for nD RV

	An nD RV is a function X:S -> V1 x V2 x ... x Vn

		S is the sample space for some random experiment

		Vi is the set of values of the i-th component Xi of X
	
			f(s) = (X1(s), X2(s), ..., Xn(s))

	The (joint) distribution of an nD RV X is the function

		P_X(v1...vn) = P(X1=v1 and X2=v2 and ... and Xn=vn)

	The marginal distributions of X are:

		P_Xi(vi*) = P(Xi=vi) = sum_{all j not i} P_X(v1,v2,...vi*,...vn)

	The expected value of an nD RV X is the expected average value of X
	over a very large number of random observations of X

		E(X) = sum_{(v1,v2,...vn)} P_X(v1...vn)*(v1,v2,...vn)

			= (E(X1), E(X2), ... E(Xn))

	The covariance matrix of an nD RV X is the matrix whose (i,j) entry is

		C(i,j) = E( (Xi-E(Xi)(Xj-E(Xj)) )

	The covariance matrix doesn't change if the Xi are shifted by constants

	If all Xi have expected value 0, then covariance in matrix form is:

		C = E( X'*X ),

	where X is assumed to be a row vector


:next time
Examples

	Do some nD examples in MATLAB

	Include examples from the Yale Face Database

