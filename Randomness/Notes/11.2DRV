
CS244, Randomness and Computation

:00
Announcements

	PS6 is up, due next Tuesday

	Second midterm exam: return at end of class


:02 (skip)
Where were we? 

	Two ways of "observing" the outcomes of a random experiment
		event E: set of outcomes (occurs if any outcomes in E occur)
			P(E) = sum of P(s) over all outcomes s in S
		RV X:S -> numbers produces value X(s) determined by outcome s
			P(X=v) is probability of the event {s in S | X(s)=v}
			collection of values P(X=v) is distribution (PDF) of X

	Observations may depend on one another
		occurrence of event E may alter probability of event F
		conditional probability P(F | E) = P(E and F)/P(E)
		E, F independent when P(F|E) = P(F), or P(E and F) = P(E)P(F)
		observation of value of RV X may affect PDF of X
		X, Y independent if distribution of each not affected by other,
		or events X=v, Y=w independent for all values v, w

	Random phenomena have deterministic qualities at the macro level
		recall random walk, number of heads in large number of tosses
		E(X) and var(X) capture some essential large-scale features


:05
Recap

        RVs represent measurements taken on a random experiment

        Average behavior of RV X over many samples described in terms of
        expected value (average) and variance (variation from average)

                E(X) = sum_i p_i v_i

                var(X) = sum_i pi (vi - E(X))^2 = E(X^2) - E(X)^2

                var(X) is squared distance; sigma(X) = sqrt(var(X)) is distance

        Chebyshev inequality bounds probability of large deviations

                P(|X - E(X)| >= k sigma(X))  <=  1 / k^2

        Law of large numbers (LLN): observed averages are probably close to E(X)

                P(|avg of X1...Xn - E(X)| < epsilon) -> 1 as n -> infinity


:10
2D RV

	RV often come in pairs

	For example, bats may have a color and a speed

	If the possible colors are red and blue and speeds are fast and slow,
	one could tabulate the probabilities of the various combinations:

				color
			blue		red

		fast	0.56		0.12

	speed

		slow	0.14		0.18

	This is an empirical version of what is called the joint distribution 
	of the pair of random variables speed, color

		show MATLAB hist3 plot using my batsGen function

		implementation forms part of PS6

	In general, the joint distribution of RV X, Y is the collection of
	probabilities P(X=vi and Y=wj) for all values vi of X, wj of Y


:20
Example of a continuous joint probability distribution

        We give the joint PDF instead of the joint PMF

        This PDF looks like a single hill in 2D (v,w) space

                f(v,w) = 1/(2 pi) * exp(-(v^2+w^2)/2)

        MATLAB will generate 2D data distributed according to this PDF:

                values = randn(10^5, 2);

        Plot
                scatter(values(:,1),values(:,2))


:25
Marginal distributions of a 2D RV

	Suppose that f(v,w) is the joint distribution of X, Y, that is:

		P(X=v and Y=w) = f(v,w)

	We can then extract the PMF of each of X, Y from f:

		P(X=v) = sum_w P(X=v and Y=w) by the partition principle
			= sum_w f(v,w)

		P(Y=w) = sum_v P(X=v and Y=w) by the partition principle
			= sum_v f(v,w)

	These summed functions are known as the marginal distributions
	("written in the margins")

	One says that Y is "marginalized out" from f(v,w) in order to get fX(v),
	and, likewise, that X is marginalized out from f(v,w) to get fY(w)

	Marginalization is similar for continuous RV, with integrals instead


:33
An example of discrete marginal distributions and independence

	Bats: recall the joint distribution of speed and color:

				color
			blue		red

		fast	0.56		0.12

	speed

		slow	0.14		0.18


	marginal distribution of speed: marginalize color out, fixing speed:

		P(fast) = P(fast and blue) + P(fast and red)
			= 0.56 + 0.12 = 0.68

		P(slow) = P(slow and blue) + P(slow and red)
			= 0.14 + 0.18 = 0.32

	marginal distribution of color: marginalize speed out, fixing color:

		P(blue) = P(fast and blue) + P(slow and blue)
			= 0.56 + 0.14 = 0.70

		P(red) = P(fast and red) + P(slow and red)
			= 0.12 + 0.18 = 0.30

	you can now easily verify that speed and color are not independent here


:45
An example of continuous marginal distributions and independence

	Consider the particular 2D Gaussian distribution discussed earlier:

		f(v,w) = 1/(2 pi) * exp(-(v^2+w^2)/2)

	We extract fX(v) by marginalizing out w (i.e., integrating out w):

		fX(v) = 1/(2pi) * int_{-infty,infty} exp(-(v^2+w^2)/2) dw

			= 1/(2pi) * exp(-(v^2/2) int_{-infty,infty} exp(-(w^2)/2)

	We know the form of a 1D Gaussian distribution with variance 1:

		1/sqrt(2 pi) exp(-(w^2/2))

	Therefore, that part integrates to 1, leaving the following:

		fX(v) = 1/sqrt(2 pi) * exp(-(v^2/2))

	which is just a 1D Gaussian with mean 0 and variance 1

	Similarly, integrating v out yields:

		fY(w) = 1/sqrt(2 pi) * exp(-(w^2/2))

	We see also that X and Y are independent here


:55 (skip)
If the joint distribution factors into functions of a single variable,
are X, Y necessarily independent?

	Let's answer this...

	Suppose that the joint distribution P(X=v and Y=w) of X, Y factors
	into the product of two distributions, one dependent on v, and the
	other on w. Are X and Y necessarily independent? 

		First ask whether P(X=v,Y=w) = f(v)*g(w) implies that
		X, Y must have the PMF f, g respectively

		P(X=v) = sum_w P(X=v,Y=w) by partition principle
			= sum_w f(v)*g(w)
			= f(v)*sum_w g(w)
			= f(v)

		so, yes!

		Therefore, if P(X=v,Y=w) = f(v)*g(w), then f is the PMF of X,
		and g is the PMF of Y, and X,Y are independent


:60 (better to leave for next time)
Covariance: a measure of dependence between two RV

	Recall that the variance of a RV Z is the expected average of 
	the squared deviation of Z from its mean:

		var(Z) = E( (Z-E(Z))(Z-E(Z)) )

	Similarly, the covariance of two RV X and Y is the expected average of 
	the product of their deviations from their respective means:

		cov(X,Y) = E( (X-E(X))(Y-E(Y)) )

	In particular,

		cov(X,X) = var(X)


:70
Covariance in terms of two expected values

	Since 

		(X - E(X))(Y - E(Y)) = XY - XE(Y) - E(X)Y + E(X)E(Y)

	and since the expected value is linear,
	the covariance of X, Y may be written as

		E( (X-E(X))(Y-E(Y)) ) = E(XY) - E(X)E(Y) 

	Notice that the covariance of two independent RV X,Y is therefore 0

	The last term in the var(X+Y) formula is just the covariance:

		var(X + Y) = var(X) + var(Y) + 2cov(X,Y)

