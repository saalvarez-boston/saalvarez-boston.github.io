
CS244, Randomness and Computation
Prof. Alvarez


n-D random variables and principal components analysis in high-dimensional spaces


n-dimensional random variables

	We can consider n RVs together: X1, X2, ... Xn, for n >= 3

	nD RV are natural models in several situations:

		time-dependent sequences 

		images

	We can consider such a collection of RVs to be a single vector

		X = [X1 X2 ... Xn]'


Examples

	Here's a 3D example in MATLAB

		A = [1 2 3; 10 -5 -2; 1 4 5]

		xyz = randn(10^3,3)*A;

		scatter3 shows data are nearly 2D within 3D

		can find vectors that generate that plane by using PCA

	We also examined sample images from the Yale Face Database

		image size is 243 x 320, which requires about 80K bytes in raw format

		if each pixel considered to be a variable, we have an 80K-dimensional space


Basic notions for nD RV

	An nD RV is a function X:S -> V1 x V2 x ... x Vn

		S is the sample space for some random experiment

		Vi is the set of values of the i-th component Xi of X
	
			f(s) = (X1(s), X2(s), ..., Xn(s))

	The (joint) distribution of an nD RV X is the function

		P_X(v1...vn) = P(X1=v1 and X2=v2 and ... and Xn=vn)

	The marginal distributions of X are:

		P_Xi(vi*) = P(Xi=vi) = sum_{all j not i} P_X(v1,v2,...vi*,...vn)

	The expected value of an nD RV X is the expected average value of X
	over a very large number of random observations of X

		E(X) = sum_{(v1,v2,...vn)} P_X(v1...vn)*(v1,v2,...vn)

			= (E(X1), E(X2), ... E(Xn))

	The covariance matrix of an nD RV X is the matrix whose (i,j) entry is

		C(i,j) = E( (Xi-E(Xi)(Xj-E(Xj)) )

	The covariance matrix doesn't change if the Xi are shifted by constants

	If all Xi have expected value 0, then covariance in matrix form is:

		C = E( X*X' ),

	where X is assumed to be a column vector, with one variable per row


Instancewise covariance

	We'll assume that we have a large sample of m observations of an nD RV X

	We'll use the notation X for the sample as well

	Each of the n rows of the sample X represents a specific variable,
	
	Each observation (value of the nD RV) corresponds to one of the m columns of the matrix X

	We'll assume that all variables (rows) of X have mean 0

		if not, subtract the means first

	Notice that the covariance is a rescaled version of:

		C = X*X'

	(the expected value here is reduced to rescaling by m, because we are working with a sample;
	note very carefully that this implicitly reflects the underlying joint probability distribution
	of the nD RV X because the observations, that is, the columns of X, are presumably sampled
	according to that joint distribution)


Toward finding the principal components (vectors)

	We know that the eigenvectors of the covariance matrix of X determine the principal axes of X

	Since the covariance matrix is basically X*X', We are interested in vectors v that satisfy the following:

		X*X'*v = c*v

	However, X*X' is an enormous matrix if n is large (n=80K, for example), making direct computation difficult

	Therefore, we first compute the following matrix, called instancewise covariance:

		B = X'*X

	this instancewise covariance matrix tabulates relationships between pairs of instances

	Notice that B is a square matrix of size n x n, where n = # instances
	and therefore is much smaller under the stated assumption


Eigenvectors of covariance matrix from eigenvectors of instancewise covariance

	Suppose that v is an eigenvector of the instancewise covariance matrix:

		X'*X*v = cv and v nonzero

	Consider the new vector w defined as follows:

		w = X*v

	Then

		X*X'*w = X*X'*X*v = X*(cv) = cX*v = cw

	In other words, w is an eigenvector of the covariance matrix X*X',
	and with the same eigenvalue as v


Principal Components Analysis for high-dimensional data

	Idea is similar to 2D:

		the principal axes are the eigenvectors of the covariance matrix

	Implementation differs in that the trick explained above using the instancewise
	covariance matrix is used to obtain the eigenvectors

	Important points:

		since data will be variable-normalized to mean 0,
		one of the eigenvectors will correspondingly be 0
		and should therefore be removed because of numerical
		error when standardizing vector lengths (see below)


Higher-D PCA by example: imagewise covariance

	We used 15 images from the Yale Face Database

	Place images in a matrix X with one image per column (raster scan to convert image to 1D vector)

	Common width and height stored in variables w and h

	Crop images for improved results

	Calculate mean for each pixel 

		means = mean(X,2);

	Calculate eigenvectors of mean-centered imagewise covariance:

		[vecs vals] = eig((X-means)'*(X-means))

	Note that smallest eigenvalue is 0

	Sort in increasing order of eigenvalues


Higher-D PCA by example: eigenvectors of the pixelwise covariance

	Transform into eigenvectors of the pixelwise covariance:

		imvecs = (X-means)*vecs;

	Pixelwise eigenvectors are already mutually orthogonal

	Normalize eigenvectors:

		for i=1:15
			imvecs(:,i) = imvecs(:,i) / norm(imvecs(:,i));
		end

	Now the pixelwise eigenvectors form an orthonormal set

	Careful with the last eigenvector - you've normalized a "noise" vector,
	so the values are not to be trusted


Higher-D PCA by example: projection and reconstruction

	Pick an image and project it onto the eigenvectors:

		v = X(:,1);

		proj = imvecs(:,1:14)'*(v-means);

	Reconstruct the image from its projections:

		vrec = means + imvecs(:,1:14)*proj;

	This recovers the original image exactly


Higher-D PCA by example: compression by keeping only largest eigenvalues

	Repeat above, but now using only the first k eigenvectors, k < 14

	For example, with the first three eigenvectors:

		proj = imvecs(1:3)'*(v-means);

	Reconstruct the image from its projections:

		vrec = means + imvecs(:,1:3)*proj;

	This provides an approximate reconstruction

